name: Scrape URLs in Batches

on:
  workflow_dispatch:
  schedule:
    - cron: '0 1 * * *'  # Runs every day at 1 AM UTC (optional)

jobs:
  scrape:
    runs-on: ubuntu-latest

    strategy:
      fail-fast: false            # Allow other jobs to continue even if one fails
      max-parallel: 4             # Adjust parallelism according to GitHub free limits
      matrix:
        subset_file: [sets_1.txt, sets_2.txt, sets_3.txt, sets_4.txt]  # Add all subset files here

    steps:
    - name: Checkout code
      uses: actions/checkout@v2

    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.8'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Run scraping script
      env:
        BIGQUERY_PROJECT_ID: ${{ secrets.BIGQUERY_PROJECT_ID }}
        GOOGLE_APPLICATION_CREDENTIALS: bigquery-key.json  # If using service account credentials
      run: |
        # Run the scraping script for this subset
        python tcg_scraping_script.py --urls-file "${{ matrix.subset_file }}"
