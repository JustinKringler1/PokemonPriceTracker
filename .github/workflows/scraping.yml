name: Scrape URLs in Batches

on:
  workflow_dispatch:
  schedule:
    - cron: '0 1 * * *'  # Runs every day at 1 AM UTC (optional)

jobs:
  scrape:
    runs-on: ubuntu-latest

    strategy:
      fail-fast: false
      max-parallel: 4
      matrix:
        subset_file: [sets_1.txt, sets_2.txt, sets_3.txt, sets_4.txt]

    steps:
    - name: Checkout code
      uses: actions/checkout@v2

    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.8'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pandas google-cloud-bigquery playwright

    # Write the credentials to a JSON file from the secret
    - name: Set up BigQuery credentials
      env:
        BIGQUERY_CREDENTIALS: ${{ secrets.BIGQUERY_CREDENTIALS }}
      run: |
        echo "$BIGQUERY_CREDENTIALS" > bigquery-key.json

    - name: Run scraping script
      env:
        BIGQUERY_PROJECT_ID: ${{ secrets.BIGQUERY_PROJECT_ID }}
        GOOGLE_APPLICATION_CREDENTIALS: bigquery-key.json
      run: |
        python tcg_scraping_script.py --urls-file "${{ matrix.subset_file }}"
